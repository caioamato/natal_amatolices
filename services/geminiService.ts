import { GoogleGenAI } from "@google/genai";
import { ScenarioType } from "../types";

const PROMPT_BASE = `Atue como um fotógrafo profissional. Crie uma fotografia realista baseada na imagem de referência enviada. 
É OBRIGATÓRIO manter estritamente a identidade facial, estrutura óssea, etnia e idade da pessoa da referência. 
A iluminação deve interagir realisticamente com o rosto da pessoa. `;

const CENARIO_RENA_DIVERTIDA = `Estilo 'candid shot' de ação hilária, com flash direto. A pessoa está montada de forma extremamente desajeitada e precária nas costas de uma rena de verdade. 
A rena parece estar correndo ou pulando, e a pessoa está se segurando para não cair, com uma expressão misturada de risada descontrolada e pânico. 
Ela veste um pijama de natal ridículo e está coberta de neve e luzes de natal emaranhadas. Cenário: uma rua suburbana nevada e caótica à noite.`;

const CENARIO_DUENDES_ASSUSTADOR = `Still de filme de terror atmosférico e granulado (estilo A24). A pessoa está encolhida no chão, com uma expressão de puro terror paralisado, tentando não fazer barulho. 
Ela está cercada por um grupo de 5 duendes (goblins) de Natal grotescos e malignos nas sombras. Os duendes têm sorrisos cruéis, dentes afiados e olhos brilhantes. 
Cenário: um sótão empoeirado e escuro, decorado com restos de natais antigos e quebrados. Iluminação dramática, gélida e sombria.`;

export const generateChristmasImage = async (
  imageBase64: string, 
  scenario: ScenarioType
): Promise<string> => {
  // Initialize AI client with API key directly from environment variable as per guidelines
  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
  
  const promptText = scenario === ScenarioType.SCARY 
    ? PROMPT_BASE + CENARIO_DUENDES_ASSUSTADOR 
    : PROMPT_BASE + CENARIO_RENA_DIVERTIDA;

  // Clean base64 string if it contains metadata prefix
  const cleanBase64 = imageBase64.replace(/^data:image\/(png|jpeg|jpg);base64,/, "");

  try {
    // We use gemini-2.5-flash-image or a similar model capable of image input + text -> image output
    // Note: The specific Python code in the prompt uses 'imagegeneration@006' (Imagen 2) on Vertex AI.
    // Since we are running client-side with @google/genai, we use the available Gemini multimodal model.
    // 'gemini-2.5-flash-image' is good for editing/generation.
    
    const response = await ai.models.generateContent({
      model: 'gemini-2.5-flash-image', 
      contents: {
        parts: [
          {
            text: promptText
          },
          {
            inlineData: {
              mimeType: "image/jpeg",
              data: cleanBase64
            }
          }
        ]
      }
    });

    // Extract image from response
    // The model might return text or image. We need to handle the image part.
    // For gemini-2.5-flash-image, it typically returns the image in the parts.
    
    const parts = response.candidates?.[0]?.content?.parts;
    
    if (parts) {
       for (const part of parts) {
         if (part.inlineData && part.inlineData.data) {
           return `data:image/png;base64,${part.inlineData.data}`;
         }
       }
    }

    throw new Error("No image generated by the model.");

  } catch (error) {
    console.error("Error generating image:", error);
    throw error;
  }
};